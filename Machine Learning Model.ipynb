{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Predicting Wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the data that we have gathered to fit a machine learning model to our data. We want to see how our model compares to the Pythagorean expectation. We will first analyze what model best fits our data. Then we will see if any of our variables will lower our test error if we remove them. Finally, we will feed our model training data and have it predict on test data. We will then calculate the Pythagorean expectation for that test error, then see if our model come close in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import plotly.offline as py\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.graph_objs import *\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup\n",
    "%matplotlib inline\n",
    "pd.set_option(\"max_r\", 15)\n",
    "py.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training.csv\")\n",
    "# Calculate the run differential to be passed into our models.\n",
    "training[\"Rdiff\"] = training[\"R\"] - training[\"RA\"]\n",
    "r = training['R']\n",
    "ra = training['RA']\n",
    "year = training[\"year\"]\n",
    "training = training.drop(['R', \"RA\"], axis=1)\n",
    "# Grab the wins (y) variable and then drop it from the dataframe (X).\n",
    "y = training['W']\n",
    "training = training.drop('W', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOS</th>\n",
       "      <th>ERA</th>\n",
       "      <th>WHIP</th>\n",
       "      <th>FIP</th>\n",
       "      <th>SO</th>\n",
       "      <th>Rdiff</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.52</td>\n",
       "      <td>1.451</td>\n",
       "      <td>4.83</td>\n",
       "      <td>1050</td>\n",
       "      <td>32.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.462</td>\n",
       "      <td>4.51</td>\n",
       "      <td>1091</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2</td>\n",
       "      <td>4.79</td>\n",
       "      <td>1.464</td>\n",
       "      <td>4.94</td>\n",
       "      <td>877</td>\n",
       "      <td>-113.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.517</td>\n",
       "      <td>5.46</td>\n",
       "      <td>846</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.375</td>\n",
       "      <td>4.48</td>\n",
       "      <td>947</td>\n",
       "      <td>-32.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SOS   ERA   WHIP   FIP    SO  Rdiff  1997  1998  1999  2000  ...   2007  \\\n",
       "0  0.0  4.52  1.451  4.83  1050   32.4     1     0     0     0  ...      0   \n",
       "1  0.0  4.49  1.462  4.51  1091   16.2     0     1     0     0  ...      0   \n",
       "2 -0.2  4.79  1.464  4.94   877 -113.4     0     0     1     0  ...      0   \n",
       "3  0.1  5.00  1.517  5.46   846  -16.2     0     0     0     1  ...      0   \n",
       "4  0.2  4.20  1.375  4.48   947  -32.4     0     0     0     0  ...      0   \n",
       "\n",
       "   2008  2009  2010  2011  2012  2013  2014  2015  2016  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year needs to be classified as a categorical variable.\n",
    "X = pd.get_dummies(training[\"year\"]).drop(1996, axis=1)\n",
    "training = pd.concat([training, X], axis=1).drop(\"year\",axis=1)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1: Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.528770245915684"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training, y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2: Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe696ff2940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe694c01710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Determine which polynomial model is the best.\n",
    "df = training.copy()\n",
    "model = LinearRegression()\n",
    "cross_val = []\n",
    "Rdiffs = []\n",
    "for i in range (2, 30):\n",
    "    df[\"Rdiff^%s\" % i] = df[\"Rdiff\"] ** i\n",
    "    Rdiffs.append(\"Rdiff^%s\" % i)\n",
    "    cross_val.append(-cross_val_score(model, df[Rdiffs], y, cv=10, scoring=\"neg_mean_squared_error\").mean())\n",
    "    \n",
    "cross_val = pd.DataFrame(cross_val, columns=[\"cross_val_score\"]) \n",
    "cross_val[\"pow\"] = cross_val.index+1\n",
    "plt.scatter(cross_val[\"pow\"], cross_val[\"cross_val_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above, we plot the cross value score for powers 1 through 30 in order to see which polynomial model minimize the test errror best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_val_score    60.749717\n",
       "pow                 1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_val_score</th>\n",
       "      <th>pow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.749717</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cross_val_score  pow\n",
       "3        60.749717    4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val[cross_val[\"cross_val_score\"] == cross_val[\"cross_val_score\"].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe68f37d390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFclJREFUeJzt3X+wXOV93/H3B3FtbosbxeG6BoEiJ3EVbBNLk1uGKWlt\nKzSidmIzjlM3bRimtatJJm3BxXKB/OGQicc4ykDcX3/QwpTM0AQaZJyhdRVNAmPjqYUlEMhEVmwn\n2LGkiXDDHUOtYBDf/rHnOvpx99fV7v1x9v2auaPdZ/fs/R7N7mef+5znnCdVhSRp9TtnuQuQJI2G\ngS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktcS5S/nLLrjggtqwYcNS/kpJWvX2\n7dv3raqa6fe8JQ30DRs2sHfv3qX8lZK06iX5+iDP6zvkkuS8JI8leTLJ00lubdqT5GNJ/iTJwST/\n5myLliQt3iA99BeBLVX1QpIp4NEknwEuBS4BfrSqXknyunEWKknqrW+gV+dyjC80d6eanwJ+Cfin\nVfVK87xj4ypSktTfQLNckqxJsh84Buyuqj3ADwPvT7I3yWeSvHGchUqSehso0KvqRFVtAi4GLk/y\nFuDVwF9V1SzwX4C7F9o2ybYm9Pc+++yzo6pbknSaDLvARZKPAv8P+CBwdVU9kyTAXFV9X69tZ2dn\na9hZLg8+cZgduw5xZO44F62dZvvWjVyzed1QryFJq1mSfU3nuadBZrnMJFnb3J4GrgK+DDwIbGme\n9jbgTxZf7sIefOIwN+88wOG54xRweO44N+88wINPHB71r5KkVW+QIZcLgYeTPAV8kc4Y+kPAbcDP\nJjkAfJxOj32kduw6xPGXTpzSdvylE+zYdWjUv0qSVr1BZrk8BWxeoH0OeNc4ipp3ZO74UO2SNMlW\n9LVcLlo7PVS7JE2yFR3o27duZHpqzSlt01Nr2L514zJVJEkr15Jey2VY87NZnOUiSf2t6ECHTqgb\n4JLU34oecpEkDc5Al6SWMNAlqSUMdElqCQNdklrCQJekljDQJaklDHRJagkDXZJawkCXpJYw0CWp\nJQx0SWoJA12SWmKQNUXPS/JYkieTPJ3k1tMe/w9JXhhfiZKkQQxy+dwXgS1V9UKSKeDRJJ+pqi8k\nmQXWjrdESdIg+vbQq2O+Bz7V/FSSNcAO4CNjrE+SNKCBxtCTrEmyHzgG7K6qPcC/An6/qo6Os0BJ\n0mAGWrGoqk4Am5KsBT6V5B8APwe8vd+2SbYB2wDWr1+/+EolST0NNculquaAR4B3AD8CfDXJM8Df\nSPLVLtvcWVWzVTU7MzNzluVKkroZZJbLTNMzJ8k0cBWwr6peX1UbqmoD8J2q+pHxlipJ6mWQIZcL\ngXuag6DnAPdX1UPjLUuSNKy+gV5VTwGb+zzn/JFVJElaFM8UlaSWMNAlqSUMdElqCQNdklrCQJek\nljDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJek\nlhhkTdHzkjyW5MkkTye5tWm/N8mhJF9KcneSqfGXK0nqZpAe+ovAlqp6K7AJuDrJFcC9wI8ClwHT\nwAfHVqUkqa9B1hQt4IXm7lTzU1X1v+afk+Qx4OKxVChJGshAY+hJ1iTZDxwDdlfVnpMemwKuBf53\nl223JdmbZO+zzz47ipolSQsYKNCr6kRVbaLTC788yVtOevg/A5+tqs912fbOqpqtqtmZmZmzr1iS\ntKChZrlU1RzwCHA1QJKPAjPAvx15ZZKkoQwyy2Umydrm9jRwFfDlJB8EtgI/X1WvjLdMSVI/fQ+K\nAhcC9yRZQ+cL4P6qeijJy8DXgf+TBGBnVf3a+EqVJPUyyCyXp4DNC7QP8mUgSVoinikqSS1hoEtS\nSxjoktQSBroktYSBLkktYaBLUksY6JLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtS\nSxjoktQSBroktcQgS9Cdl+SxJE8meTrJrU37G5LsSfKVJPcledX4y5UkdTNID/1FYEtVvRXYBFyd\n5ArgE8AdVfVG4DngA+MrU5LUT99Ar44XmrtTzU8BW4Dfa9rvAa4ZS4WSpIEMNIaeZE2S/cAxYDfw\nNWCuql5unvJNYN14SpQkDWKgQK+qE1W1CbgYuBy4dKGnLbRtkm1J9ibZ++yzzy6+UklST0PNcqmq\nOeAR4ApgbZJzm4cuBo502ebOqpqtqtmZmZmzqVWS1MMgs1xmkqxtbk8DVwEHgYeB9zVPuw749LiK\nlCT1d27/p3AhcE+SNXS+AO6vqoeS/DHwu0l+HXgCuGuMdUqS+ugb6FX1FLB5gfY/pTOeLklaATxT\nVJJawkCXpJYw0CWpJQx0SWoJA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJekljDQJaklDHRJagkD\nXZJawkCXpJYw0CWpJQx0SWoJA12SWmKQNUUvSfJwkoNJnk5yfdO+KckXkuxPsjeJqxdJ0jIaZE3R\nl4Ebq+rxJK8B9iXZDfwGcGtVfSbJO5v7bx9fqZKkXgZZU/QocLS5/XySg8A6oIC/1Tzt+4Aj4ypS\nktTfID3070mygc6C0XuAG4BdSX6TztDN3+uyzTZgG8D69evPolRJUi8DHxRNcj7wAHBDVX0b+CXg\nQ1V1CfAh4K6FtquqO6tqtqpmZ2ZmRlGzJGkBAwV6kik6YX5vVe1smq8D5m//D8CDopK0jAaZ5RI6\nve+DVXX7SQ8dAd7W3N4CfGX05UmSBjXIGPqVwLXAgST7m7ZbgH8JfDLJucBf0YyTS5KWxyCzXB4F\n0uXhHx9tOZKkxfJMUUlqCQNdklrCQJekljDQJaklDHRJagkDXZJawkCXpJYw0CWpJQx0SWoJA12S\nWsJAl6SWMNAlqSWGWrFopXnwicPs2HWII3PHuWjtNNu3buSazeuWuyxJWharNtAffOIwN+88wPGX\nTgBweO44N+88AGCoS5pIq3bIZceuQ98L83nHXzrBjl2HlqkiSVpeqzbQj8wdH6pdktpukCXoLkny\ncJKDSZ5Ocv1Jj/3rJIea9t8Yb6mnumjt9FDtktR2g/TQXwZurKpLgSuAX07ypiTvAN4D/FhVvRn4\nzTHWeYbtWzcyPbXmlLbpqTVs37pxKcuQpBVjkCXojgJHm9vPJzkIrKOzpuhtVfVi89ixcRZ6uvkD\nn85ykaSOoWa5JNkAbAb2ADuAv5/kY3QWif5wVX1x1AX2cs3mdQa4JDUGDvQk5wMPADdU1beTnAt8\nP51hmL8L3J/kh6qqTttuG7ANYP369SMrXJJ0qoFmuSSZohPm91bVzqb5m8DO6ngMeAW44PRtq+rO\nqpqtqtmZmZlR1S1JOs0gs1wC3AUcrKrbT3roQWBL85y/A7wK+NY4ipQk9TfIkMuVwLXAgST7m7Zb\ngLuBu5N8CfgucN3pwy2SpKUzyCyXR4F0efgXRluOJGmxVu2ZopKkUxnoktQSBroktYSBLkktYaBL\nUksY6JLUEga6JLWEgS5JLWGgS1JLGOiS1BIGuiS1hIEuSS1hoEtSSxjoktQSBroktYSBLkktYaBL\nUksMsqboJUkeTnIwydNJrj/t8Q8nqSRnLBAtSVo6g6wp+jJwY1U9nuQ1wL4ku6vqj5NcAvxD4Btj\nrVKS1FffHnpVHa2qx5vbzwMHgXXNw3cAHwFcHFqSltlQY+hJNgCbgT1J3g0crqonx1CXJGlIgwy5\nAJDkfOAB4AY6wzC/AvzUANttA7YBrF+/fnFVSpL6GqiHnmSKTpjfW1U7gR8G3gA8meQZ4GLg8SSv\nP33bqrqzqmaranZmZmZ0lUuSTtG3h54kwF3Awaq6HaCqDgCvO+k5zwCzVfWtMdUpSepjkB76lcC1\nwJYk+5ufd465LknSkPr20KvqUSB9nrNhVAVJkhZn4IOiq8mDTxxmx65DHJk7zkVrp9m+dSPXbF7X\nf0NJWsVaF+gPPnGYm3ce4PhLJwA4PHecm3ceADDUJbVa667lsmPXoe+F+bzjL51gx65Dy1SRJC2N\n1gX6kbnjQ7VLUlu0LtAvWjs9VLsktUXrAn371o1MT605pW16ag3bt25cpookaWm07qDo/IFPZ7lI\nmjStC3TohLoBLmnStDLQJWklWOpzYgx0STpLCwU3sOTnxBjoknQWup3MeN7UOV3PiTHQJWmZLdQT\n73Yy4+lt88Z5ToyBLkkD6NYT7xbc3YzznBgDXZJO0u1AZree+JqEE3Xmssprp6d48eVXTtlm3OfE\nGOiSWq1bQA97ILPbUMmJKqan1pwR3L/67jcDS3tOTGqBb5ZxmZ2drb179y7Z75M02U4fJoFO2P7s\nj6/jgX2Hz2g/b+ocnvvOS2e8zrpmmOTwAqG+7qSx9HEFd5J9VTXb73mDLEF3CfDbwOuBV4A7q+qT\nSXYAPwN8F/ga8M+rau7sypak4Q07TPI7e/78jGGSfgcy73j/pgW/HOZ/10o4mXGQIZeXgRur6vEk\nrwH2JdkN7AZurqqXk3wCuBn4d2OsVZLO0GsNhF7DJMO4aO30qrisyCBL0B0Fjja3n09yEFhXVX9w\n0tO+ALxvPCVKUscw0wZ37DrERWunFxwmWeyBzJXSE+9mqIOiSTYAm4E9pz30L4D7RlPS+PQ6Dddl\n66SVbdhpg72GSbqNoS/HgcxRGjjQk5wPPADcUFXfPqn9V+gMy9zbZbttwDaA9evXn1WxZ6PXn2Ww\n9KfoSupumJ54t952v2GS2R98bdfgXq2f+4FmuSSZAh4CdlXV7Se1Xwf8IvCTVfWdfq+znLNcrrzt\nj7oeoYbuR68/f9OWsdcmTaJe0wkX6lX3OoFnoWmDH3/vZas2mE83ylkuAe4CDp4W5lfTOQj6tkHC\nfLktZmm6I3PHHYqRBjSq+d7D9sSXYtrgatG3h57kJ4DPAQfoTFsEuAX498Crgf/btH2hqn6x12ut\nth56twMkbfrml0ZhlPO9j8wdp1sqtb0n3s2gPfSJObGo2xvu4++9DGDBx3q96RyKUZsN09u+ZvO6\nrh2mbr3qbgJdZ6ZMck98ZEMubTHIHNLTH/vQffsXfK1xXi1NWm7dJhDs/fpfntLbHtd87+1bN674\nE3hWqonpoS9Gr2GaXj10x921WnSbTTJMb7vXsOWw873nh0/8DJ3KIZcR6DdMM8wR+kkY59PKNMrZ\nJAsJDD3fu9dnSGdyyGUEug3TwPBH6Me5SonUTa/zL4adTeJ875XPHvoi9BqK6XaEPsCf3fausdem\nyTXM8MliZpP06m0bzOM1aA/9nKUopm16zWnvthrJOFcpkeZ74oebkJ7viS8U5tD7vbpu7TQff+9l\nrFs7TU66/+vXXLZgu2G+cjjksgjdplX1O0LvgR6Ny2JOi1/MbBJnmaxsBvoi9PsgwHDj7n5ANIyF\nOgbDrqbT673q+3H1cgx9kYbtbS92CqTabdgTeLrNTOl1EtyknozTJs5yGbNh//RczLVkVqNhA2pS\nDHM9k14n8HQbWnn1uef07IlP0v/1JLOHvkQmoYc+7PU8lvIkksV80Sym9zyKXnWvE3h6zaK64/2b\nJvpLs808sWiFadsJR6M6w7Db8YhR/r8s5osGFr6+T7dter1Wt/+XYfW7zklbOgY6k4G+Ao2yJ7oU\nr7UUZxiO+kJMy3Uq+2J61d0s9xegVh4DfRUZ9s9+WLj3uJgP9WJ6r8MG5GLDrtulUmG4yy4s5osG\nGCqEe71Wty+tbtczWQlDVFpZDPRVYikv6zuq3usozzAc9vf3uqjTKL9oYHQ99F69auj+5WRoa56B\nvkosZuGNbnodGFvuYZJxD9+M+osGRjeGbkDrbBnoq8QbbvqfXWctwHB/9i9V73XU47jD/OXQzai/\naLrVtZhZLtLZGlmgJ7kE+G3g9XSWoLuzqj6Z5LXAfcAG4BngH1fVc71ey0A/0yiXxhvlsl7L3eNc\n7Ak0HjBUG43y4lwvAzdW1aXAFcAvJ3kTcBPwh1X1RuAPm/sa0vatG5meWnNK2/wJId0e+9V3v3nB\niyTNLRB0MJ4LMV2zeR2fv2kLf3bbu/j8TVtGHpjXbF634O//6M+8uev/V7dtDHNNiqGHXJJ8GviP\nzc/bq+pokguBR6pqY69t7aEvbDF/9i+kV2+/Tb1XhzY0acYyhp5kA/BZ4C3AN6pq7UmPPVdV399r\newN9vPqdvGQQSqvTyK/lkuR84AHghqr6dpJ+m8xvtw3YBrB+/fpBf50Wod/V87ymh9RuA/XQk0wB\nDwG7qur2pu0QDrlI0tiN7KBoOl3xu4CD82He+H3guub2dcCnF1OoJGk0BhlyuRK4FjiQZH/Tdgtw\nG3B/kg8A3wB+bjwlSpIG0TfQq+pR/vo8l9P95GjLkSQtlotES1JLGOiS1BJLei2XJM8CX+/ztAuA\nby1BOSvRJO87TPb+u++Ta5D9/8Gqmun3Qksa6INIsneQ6TltNMn7DpO9/+77ZO47jHb/HXKRpJYw\n0CWpJVZioN+53AUso0ned5js/XffJ9fI9n/FjaFLkhZnJfbQJUmLsGICPcnVSQ4l+WqS1i+WkeTu\nJMeSfOmkttcm2Z3kK82/PS9HvFoluSTJw0kOJnk6yfVN+6Ts/3lJHkvyZLP/tzbtb0iyp9n/+5K8\narlrHZcka5I8keSh5v5E7HuSZ5IcSLI/yd6mbWTv+xUR6EnWAP8J+EfAm4Cfb1ZFarP/Blx9Wtuk\nrAI16atgvQhsqaq3ApuAq5NcAXwCuKPZ/+eADyxjjeN2PXDwpPuTtO/vqKpNJ01VHNn7fkUEOnA5\n8NWq+tOq+i7wu8B7lrmmsaqqzwJ/eVrze4B7mtv3ANcsaVFLpKqOVtXjze3n6Xyw1zE5+19V9UJz\nd6r5KWAL8HtNe2v3P8nFwLuA/9rcDxOy712M7H2/UgJ9HfDnJ93/ZtM2af52VR2FTugBr1vmesau\nWQVrM7CHCdr/ZshhP3AM2A18DZirqpebp7T5M/BbwEfoLDoP8ANMzr4X8AdJ9jWL/8AI3/cDr1g0\nZgtdzdHpNy232FWw2qCqTgCbkqwFPgVcutDTlraq8Uvy08CxqtqX5O3zzQs8tXX73riyqo4keR2w\nO8mXR/niK6WH/k3gkpPuXwwcWaZaltNfNKs/0fx7bJnrGZtmFawHgHuramfTPDH7P6+q5oBH6BxL\nWJtkvpPV1s/AlcC7kzxDZ2h1C50e+yTsO1V1pPn3GJ0v8ssZ4ft+pQT6F4E3Nke6XwX8EzorIk2a\niVgFatJXwUoy0/TMSTINXEXnOMLDwPuap7Vy/6vq5qq6uKo20Pmc/1FV/TMmYN+T/M0kr5m/DfwU\n8CVG+L5fMScWJXknnW/qNcDdVfWxZS5prJL8DvB2Olda+wvgo8CDwP3AeppVoKrq9AOnq16SnwA+\nBxzgr8dRb6Ezjj4J+/9jdA5+raHTqbq/qn4tyQ/R6bW+FngC+IWqenH5Kh2vZsjlw1X105Ow780+\nfqq5ey7w36vqY0l+gBG971dMoEuSzs5KGXKRJJ0lA12SWsJAl6SWMNAlqSUMdElqCQNdklrCQJek\nljDQJakl/j9TPHzBL2tHegAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe68f3b5470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_val = []\n",
    "n_neighbors = []\n",
    "for i in range (1, 50):\n",
    "    model = KNeighborsRegressor(n_neighbors=i)\n",
    "    n_neighbors.append(i)\n",
    "    cross_val.append(-cross_val_score(model, training, y, cv=10, scoring=\"neg_mean_squared_error\").mean())\n",
    "    \n",
    "cross_val = pd.DataFrame(cross_val, columns=[\"cross_val_score\"]) \n",
    "cross_val[\"n_neighbors\"] = cross_val.index+1\n",
    "plt.scatter(cross_val[\"n_neighbors\"], cross_val[\"cross_val_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above helps us determine the amount of neighbors that best fits the model to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_val_score    20.226727\n",
       "n_neighbors         1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_val_score</th>\n",
       "      <th>n_neighbors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20.226727</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cross_val_score  n_neighbors\n",
       "12        20.226727           13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val[cross_val[\"cross_val_score\"] == cross_val[\"cross_val_score\"].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.215745007680496"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tree.DecisionTreeRegressor()\n",
    "-cross_val_score(model, X=training, y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 5: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fe68f2f97b8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFgVJREFUeJzt3X9sXeV9x/H3p2loLYLkIFyUOMnCJhSBSku6K8qUqWrp\nIDSqIGXr1K7LMnVVhgRa0FBUAn+0o6qIli2qtlXtMoLEpHS0WkyKBGuIaKoum0hxfkBITQpFdMSO\nwBXNCKq1kvDdH/cYjHNvfI597XvOeT4vyfK9j8+593k44ePr53zPcxQRmJlZOt7T7Q6YmdnccvCb\nmSXGwW9mlhgHv5lZYhz8ZmaJcfCbmSVmyuCXtFTSPklDko5J2pi1f03SM5KOSHpc0uI2+6+X9Hz2\ntb7TAzAzs2I0VR2/pEXAoog4JOki4CCwFjgREa9n2/wVcGVE3Dpp34uBQaABRLbv70bErzo+EjMz\ny2XKT/wRcTIiDmWPTwNDQP946GcupBnsk60G9kbEa1nY7wVunHm3zcxsut5bZGNJy4GVwIHs+deB\nPwP+F/hEi136gZcnPD+RtZ3XJZdcEsuXLy/SNTOzpB08ePCXEdGXZ9vcwS9pAbALuGP8035E3APc\nI2kzcDvwlcm7tXiplnNLkjYAGwCWLVvG4OBg3q6ZmSVP0i/ybpurqkfSfJqhvzMiBlps8h3gD1u0\nnwCWTni+BBhp9R4RsT0iGhHR6OvL9UvLzMymIU9Vj4AdwFBEbJvQfvmEzW4Cnmux+x7gBkkLJS0E\nbsjazMysS/JM9awC1gFHJR3J2u4G/kLSCuAt4BfArQCSGsCtEfGliHhN0teAp7L97o2I1zo6AjMz\nK2TKcs5uaDQa4Tl+M7P8JB2MiEaebX3lrplZYhz8ZmaJKVTHX2a7Dw+zdc9xRk6Nsbi3h02rV7B2\n5ZSXDJiZJacWwb/78DCbB44y9uZZAIZPjbF54CiAw9/MbJJaTPVs3XP87dAfN/bmWbbuOd6lHpmZ\nlVctgn/k1FihdjOzlNUi+Bf39hRqNzNLWS2Cf9PqFfTMn/eutp7589i0ekWXemRmVl61OLk7fgLX\nVT1mZlOrRfBDM/wd9GZmU6vFVI+ZmeXn4DczS4yD38wsMQ5+M7PEOPjNzBLj4DczS4yD38wsMQ5+\nM7PEOPjNzBLj4DczS4yD38wsMQ5+M7PEOPjNzBLj4DczS4yD38wsMQ5+M7PEOPjNzBIzZfBLWipp\nn6QhScckbczat0p6TtIzkh6W1Ntm/5ckHZV0RNJgpwdgZmbF5PnEfwa4MyKuAK4FbpN0JbAX+GBE\nfAj4GbD5PK/xiYi4OiIaM+6xmZnNyJTBHxEnI+JQ9vg0MAT0R8TjEXEm2+xJYMnsddPMzDql0By/\npOXASuDApB99EfiPNrsF8Likg5I2FO2gmZl11nvzbihpAbALuCMiXp/Qfg/N6aCdbXZdFREjkj4A\n7JX0XET8uMXrbwA2ACxbtqzAEMzMrIhcn/glzacZ+jsjYmBC+3rg08AXIiJa7RsRI9n3V4GHgWva\nbLc9IhoR0ejr6ys2CjMzyy1PVY+AHcBQRGyb0H4j8GXgpoj4dZt9L5R00fhj4Abg2U503MzMpifP\nJ/5VwDrguqwk84ikNcA/ARfRnL45IunbAJIWS3os2/dSYL+kp4GfAI9GxA86PwwzM8tryjn+iNgP\nqMWPHmvRNj61syZ7/CLw4Zl00MzMOstX7pqZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/\nmVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVliHPxmZolx8JuZJcbB\nb2aWGAe/mVliHPxmZolx8JuZJcbBb2aWGAe/mVli3tvtDlhruw8Ps3XPcUZOjbG4t4dNq1ewdmV/\nt7tlZjXg4C+h3YeH2TxwlLE3zwIwfGqMzQNHARz+ZjZjnuopoa17jr8d+uPG3jzL1j3Hu9QjM6sT\nB38JjZwaK9RuZlbElMEvaamkfZKGJB2TtDFr3yrpOUnPSHpYUm+b/W+UdFzSC5Lu6vQA6mhxb0+h\ndjOzIvJ84j8D3BkRVwDXArdJuhLYC3wwIj4E/AzYPHlHSfOAbwKfAq4EPp/ta+exafUKeubPe1db\nz/x5bFq9oks9MrM6mTL4I+JkRBzKHp8GhoD+iHg8Is5kmz0JLGmx+zXACxHxYkT8BngIuLkzXa+v\ntSv7ue+Wq+jv7UFAf28P991ylU/smllHFKrqkbQcWAkcmPSjLwLfbbFLP/DyhOcngI8Wec9UrV3Z\n76A3s1mR++SupAXALuCOiHh9Qvs9NKeDdrbarUVbtHn9DZIGJQ2Ojo7m7ZaZmRWUK/glzacZ+jsj\nYmBC+3rg08AXIqJVoJ8Alk54vgQYafUeEbE9IhoR0ejr68vbfzMzKyhPVY+AHcBQRGyb0H4j8GXg\npoj4dZvdnwIul3SZpAuAzwGPzLzbZmY2XXnm+FcB64Cjko5kbXcD/wC8D9jb/N3AkxFxq6TFwP0R\nsSYizki6HdgDzAMeiIhjHR9FAV4KwcxSp9YzNN3VaDRicHCw4687eSkEaJZJumLGzKpO0sGIaOTZ\nNqkrd70UgplZYsHvpRDMzBILfi+FYGaWWPB7KQQzs8TW4x8/geuqHjNLWVLBD14KwcwsqakeMzNz\n8JuZJcfBb2aWGAe/mVliHPxmZolx8JuZJSa5ck7Lx6uYmtWXg9/OMXkV0+FTY2weOArg8DerAU/1\n2Dm8iqlZvTn47RxexdSs3hz8dg6vYmpWbw5+O4dXMTWrN5/ctXN4FVOzenPwn0fKJY1exdSsvhz8\nbbik0czqynP8bbik0czqysHfhksazayuHPxtuKTRzOrKwd+GSxrNrK58crcNlzSaWV05+M8jb0lj\nymWfZlY9U071SFoqaZ+kIUnHJG3M2j+bPX9LUuM8+78k6aikI5IGO9n5Mhgv+xw+NUbwTtnn7sPD\n3e6amVlLeeb4zwB3RsQVwLXAbZKuBJ4FbgF+nOM1PhERV0dE218QVeWyTzOrmimneiLiJHAye3xa\n0hDQHxF7ASTNbg9LzmWfZlY1hap6JC0HVgIHCuwWwOOSDkraUOT9qsBln2ZWNbmDX9ICYBdwR0S8\nXuA9VkXER4BP0Zwm+lib198gaVDS4OjoaIGX7y6XfVrZ7T48zKotP+Syux5l1ZYf+vyT5avqkTSf\nZujvjIiBIm8QESPZ91clPQxcQ4vzAhGxHdgO0Gg0osh7dJPLPqsh1corrzllrUwZ/GpO4u8AhiJi\nW5EXl3Qh8J7s3MCFwA3AvdPqaYl5JctySzn8zld8UPexW3t5pnpWAeuA67KSzCOS1kj6jKQTwO8B\nj0raAyBpsaTHsn0vBfZLehr4CfBoRPxgFsZh1lbKlVcuPrBW8lT17Afale483GL7EWBN9vhF4MMz\n6aDZTKUcfot7exhuMU4XH6TNa/VY7aVceeXiA2vFwW+1l3L4rV3Zz323XEV/bw8C+nt7uO+Wqzy/\nnziv1WO1l3rllYsPbDIHv5XObJReFgm/VEs/LR0OfiuVoqWXnQ7plEs/LR2e47dSKVJ6ORsro6Zc\n+mnpcPBbqRQpvZyNkE659NPS4amehFRh7rpI3flshLTr3i0F/sRfA3kW4arKDWOKlF7ORn1+yqWf\nlg4Hf8XlDfSqzF0XqTufjZB23bulwFM9FZd3Ea4qzV3nLb2crfp8171b3Tn4Ky5voNd17tohbVac\np3oqLu88t+euzWycP/FX3KbVK951wRG0DvTZmhapQqWQdZ6Pe7U5+CuuSKB3elrEV7mmyce9+hz8\nNdCteW7f3SlNPu7V5zl+m7YqVQpZ5/i4V5+D36Yt5RucpMzHvfoc/DZtrhRKk4979XmO36Yt9Ruc\npKoMx91VRTOjiOh2H87RaDRicHCw290wsxKaXFUEzb84Ul9aQ9LBiGjk2dZTPWZWKVVZd6rMHPxm\nVimuKpo5B7+ZVYqrimbOwW9mleKqoplzVY+ZVUoZqoqqzsFvZpUzG8uUpFQi6uA3s+SltvDclHP8\nkpZK2idpSNIxSRuz9s9mz9+S1LZ2VNKNko5LekHSXZ3svJlZJxQpEc1zj+uyy/OJ/wxwZ0QcknQR\ncFDSXuBZ4Bbgn9vtKGke8E3geuAE8JSkRyLipzPvejWl9OekvcPHvdzylojW5S+DKT/xR8TJiDiU\nPT4NDAH9ETEUEVNdMXEN8EJEvBgRvwEeAm6eaaerKu+N0a1efNzLL2+JaF0uHitUzilpObASOJBz\nl37g5QnPT2RtrV57g6RBSYOjo6NFulUZdflHY015/+RPbRqhivKWiNbl4rHcJ3clLQB2AXdExOt5\nd2vR1nJxoIjYDmyH5lo9eftVJXX5R2PF/uRPbRphuro5HZa3RHRxbw/DLY7nTC8em+ux5wp+SfNp\nhv7OiBgo8PongKUTni8BRgrsXyuz9Y/G5l6Ru1DlPe5F72xVp/MGZfill6dENO89rovoxtjzVPUI\n2AEMRcS2gq//FHC5pMskXQB8DnikeDfrwVcc1keRv95mYxqhbucNqjINunZlP/fdchX9vT0I6O/t\nmfGqoN0Ye55P/KuAdcBRSUeytruB9wH/CPQBj0o6EhGrJS0G7o+INRFxRtLtwB5gHvBARBzr/DCq\nwVcc1keRv95mYxqhbve9rdI0aKcvHuvG2KcM/ojYT+u5eoCHW2w/AqyZ8Pwx4LHpdrBuunVjdOus\non/yd3oaoUpBmWdKKuVp0G6M3Yu02ZypU8XKbPzJX+Q1q7JCZd4pqZSnQbsxdt+By+aE75rUWVX5\n77lqyw9bfprt7+3hv+667l1tdTpZXVQnxl7kDlxeq8fmRN3mpLutKueLikxJpTwNOtdjd/DbnKjS\nnHRVVCEoU567LzPP8ducqMqctHVWynP3ZebgtznhAEjTbJwEt5nzVI/NiarMSVvnVWFKKjUOfpsz\nDgCzcvBUj5lZYhz8ZmaJ8VSPmdVWyheFnY+D38xqqQxLPZeVp3rMrJaqstRzNzj4zayWfLV4ew5+\nM6slXy3enoPfzGrJV4u355O7ZlZLvlq8PQe/mdWWrxZvzVM9ZmaJcfCbmSXGwW9mlhgHv5lZYhz8\nZmaJcfCbmSXGwW9mlhgHv5lZYhz8ZmaJmTL4JS2VtE/SkKRjkjZm7RdL2ivp+ez7wjb7n5V0JPt6\npNMDMDOzYvJ84j8D3BkRVwDXArdJuhK4C3giIi4HnsietzIWEVdnXzd1pNdmZjZtUwZ/RJyMiEPZ\n49PAENAP3Aw8mG32ILB2tjppZmadU2iOX9JyYCVwALg0Ik5C85cD8IE2u71f0qCkJyW1/eUgaUO2\n3eDo6GiRbpmZWQG5g1/SAmAXcEdEvF7gPZZFRAP4E+Abkn6n1UYRsT0iGhHR6OvrK/DyZmZWRK5l\nmSXNpxn6OyNiIGt+RdKiiDgpaRHwaqt9I2Ik+/6ipB/R/Ivh5zPuuZlZye0+PFzK+wHkqeoRsAMY\niohtE370CLA+e7we+H6LfRdKel/2+BJgFfDTmXbazKzsdh8eZvPAUYZPjRHA8KkxNg8cZffh4W53\nLddUzypgHXDdhLLMNcAW4HpJzwPXZ8+R1JB0f7bvFcCgpKeBfcCWiHDwm1ntbd1znLE3z76rbezN\ns2zdc7xLPXrHlFM9EbEfUJsff7LF9oPAl7LH/w1cNZMOmplV0cipsULtc8lX7pqZzYLFvT2F2ueS\ng9/MbBZsWr2Cnvnz3tXWM38em1av6FKP3uGbrZuZzYLx6p0yVvU4+M3MZsnalf2lCPrJPNVjZpYY\nB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5klxsFvZpYYB7+ZWWIc/GZmiXHwm5kl\nxsFvZpYYr85pZm8r683BrbMc/GYGvHNz8PH7xI7fHBxw+NeMp3rMDCj3zcGtsxz8ZgaU++bg1lkO\nfjMDyn1zcOssB7+ZAeW+Obh1lk/umhlQ7puDW2c5+M3sbWW9Obh1lqd6zMwS4+A3M0vMlMEvaamk\nfZKGJB2TtDFrv1jSXknPZ98Xttl/fbbN85LWd3oAZmZWTJ5P/GeAOyPiCuBa4DZJVwJ3AU9ExOXA\nE9nzd5F0MfAV4KPANcBX2v2CMDOzuTFl8EfEyYg4lD0+DQwB/cDNwIPZZg8Ca1vsvhrYGxGvRcSv\ngL3AjZ3ouJmZTU+hOX5Jy4GVwAHg0og4Cc1fDsAHWuzSD7w84fmJrK3Va2+QNChpcHR0tEi3zMys\ngNzlnJIWALuAOyLidUm5dmvRFq02jIjtwPbsvUYl/WLCjy8Bfpm3rxVRtzHVbTxQvzHVbTxQvzHN\nZDy/lXfDXMEvaT7N0N8ZEQNZ8yuSFkXESUmLgFdb7HoC+PiE50uAH031fhHRN+n9ByOikaevVVG3\nMdVtPFC/MdVtPFC/Mc3VePJU9QjYAQxFxLYJP3oEGK/SWQ98v8Xue4AbJC3MTurekLWZmVmX5Jnj\nXwWsA66TdCT7WgNsAa6X9DxwffYcSQ1J9wNExGvA14Cnsq97szYzM+uSKad6ImI/refqAT7ZYvtB\n4EsTnj8APDDdDma2z3D/MqrbmOo2HqjfmOo2HqjfmOZkPIpoea7VzMxqyks2mJklpvTBL+lGSccl\nvSDpnKuDq0bSS5KOZudKBrvdn+mQ9ICkVyU9O6Et1xIeZdRmPF+VNDzpvFZlzHSplbI5z3gqe5wk\nvV/STyQ9nY3pb7L2yyQdyI7RdyVd0PH3LvNUj6R5wM9onjw+QfME8ecj4qdd7dgMSHoJaEREZWuP\nJX0MeAP414j4YNb2t8BrEbEl+wW9MCK+3M1+5tVmPF8F3oiIv+tm36YrK7FeFBGHJF0EHKR5df2f\nU8HjdJ7x/DEVPU5ZxeSFEfFGVjK/H9gI/DUwEBEPSfo28HREfKuT7132T/zXAC9ExIsR8RvgIZpL\nRVgXRcSPgcnVWXmW8CilNuOptBkutVI65xlPZUXTG9nT+dlXANcB/561z8oxKnvw517yoUICeFzS\nQUkbut2ZDsqzhEfV3C7pmWwqqBJTIq1MY6mVUps0HqjwcZI0T9IRmhfA7gV+DpyKiDPZJrOSeWUP\n/txLPlTIqoj4CPApmiudfqzbHbKWvgX8DnA1cBL4++52Z3omL7XS7f7MVIvxVPo4RcTZiLia5qoG\n1wBXtNqs0+9b9uA/ASyd8HwJMNKlvnRERIxk318FHqZ5sOvglWwednw+ttUSHpUREa9k/1O+BfwL\nFTxO51tqJft5pY5Tq/HU4TgBRMQpmsvZXAv0Shq/xmpWMq/swf8UcHl2lvsC4HM0l4qoJEkXZiem\nkHQhzSUsnj3/XpWRZwmPyhgPx8xnqNhxmuFSK6XTbjxVPk6S+iT1Zo97gD+gee5iH/BH2WazcoxK\nXdUDkJVnfQOYBzwQEV/vcpemTdJv0/yUD82rpr9TxfFI+jeai+9dArxC82Y7u4HvAcuA/wE+W5Xl\nOdqM5+M0pw8CeAn4y/G58SqQ9PvAfwJHgbey5rtpzotX7jidZzyfp6LHSdKHaJ68nUfzQ/j3IuLe\nLCceAi4GDgN/GhH/19H3Lnvwm5lZZ5V9qsfMzDrMwW9mlhgHv5lZYhz8ZmaJcfCbmSXGwW9mlhgH\nv5lZYhz8ZmaJ+X+aTyWZRMnIKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe68f3bfa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cross_val = []\n",
    "n_estimators = []\n",
    "for i in range (10, 40):\n",
    "    model = RandomForestRegressor(n_estimators=i)\n",
    "    n_estimators.append(i)\n",
    "    cross_val.append(-cross_val_score(model, training, y, cv=10, scoring=\"neg_mean_squared_error\").mean())\n",
    "    \n",
    "cross_val = pd.DataFrame(cross_val, columns=[\"cross_val_score\"]) \n",
    "cross_val[\"n_estimators\"] = cross_val.index+1\n",
    "plt.scatter(cross_val[\"n_estimators\"], cross_val[\"cross_val_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above helps us determine the best amount of estimators for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_val_score    20.144058\n",
       "n_estimators        1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cross_val_score</th>\n",
       "      <th>n_estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.144058</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cross_val_score  n_estimators\n",
       "20        20.144058            21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val[cross_val[\"cross_val_score\"] == cross_val[\"cross_val_score\"].min()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have examined various models, we can safely conclude that the best model for this data is linear regresssion.\n",
    "\n",
    "Here we are testing our best model with different combinations of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.472625817114366"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping strikeouts improves our score by .02\n",
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training.drop(\"SO\",axis=1), y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.385170574685219"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping strikeouts and strength of schedule.\n",
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training.drop([\"SO\",\"SOS\"],axis=1), y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.382107165558104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping strikeouts, strength of schedule, and WHIP.\n",
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training.drop([\"SO\",\"SOS\",\"WHIP\"],axis=1), y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.292438000122736"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping strikeouts, strength of schedule, WHIP, ERA.\n",
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training.drop([\"SO\",\"SOS\",\"WHIP\",\"ERA\"],axis=1), y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.835073004006638"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping strikeouts, strength of schedule, WHIP, ERA, and FIP.\n",
    "# This worsens our model.\n",
    "model = LinearRegression()\n",
    "-cross_val_score(model, X=training.drop([\"SO\",\"SOS\",\"WHIP\",\"ERA\",\"FIP\"],axis=1), y=y, cv=10, scoring=\"neg_mean_squared_error\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we conclude that the Linear Regression model utilizing Fielding-Independent Pitching, Run Differential, and the year is our best regression model based on our calculated cross val scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIP</th>\n",
       "      <th>Rdiff</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.83</td>\n",
       "      <td>32.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.51</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.94</td>\n",
       "      <td>-113.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.46</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.48</td>\n",
       "      <td>-32.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    FIP  Rdiff  1997  1998  1999  2000  2001  2002  2003  2004  ...   2007  \\\n",
       "0  4.83   32.4     1     0     0     0     0     0     0     0  ...      0   \n",
       "1  4.51   16.2     0     1     0     0     0     0     0     0  ...      0   \n",
       "2  4.94 -113.4     0     0     1     0     0     0     0     0  ...      0   \n",
       "3  5.46  -16.2     0     0     0     1     0     0     0     0  ...      0   \n",
       "4  4.48  -32.4     0     0     0     0     1     0     0     0  ...      0   \n",
       "\n",
       "   2008  2009  2010  2011  2012  2013  2014  2015  2016  \n",
       "0     0     0     0     0     0     0     0     0     0  \n",
       "1     0     0     0     0     0     0     0     0     0  \n",
       "2     0     0     0     0     0     0     0     0     0  \n",
       "3     0     0     0     0     0     0     0     0     0  \n",
       "4     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct training dataframe to include only the variables that make the best model.\n",
    "training = training.drop([\"SO\",\"SOS\",\"WHIP\",\"ERA\"], axis=1)\n",
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting wins using our new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in order to calcualte pythagorean expectation\n",
    "def calc_win_ratio(r, ra):\n",
    "    return (r**2) / (r**2 + ra**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "y_resids = []\n",
    "pythep_resids = []\n",
    "pred = pd.Series()\n",
    "y_test = pd.Series()\n",
    "\n",
    "# Simulate calculations 100 times so we can get long-run averages for residuals.\n",
    "for i in range(1,101):\n",
    "    # Randomly split data into training and test sets.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(training, y, test_size=300)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_predicted = model.predict(X_test)\n",
    "    \n",
    "    y_resid = abs(y_test.values - y_predicted)\n",
    "    \n",
    "    ra_test = ra[X_test.index]\n",
    "    r_test = r[X_test.index]\n",
    "    \n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    X_test[\"pythep\"] = calc_win_ratio(r_test, ra_test) * 162\n",
    "    X_test[\"year\"] = year[X_test.index]\n",
    "    \n",
    "    # Get one point from each year in the data set, so we can make a plot.\n",
    "    years = []\n",
    "    indicies = []\n",
    "    for i in X_test.index:\n",
    "        if X_test.loc[i, \"year\"] not in years:\n",
    "            years.append(X_test.loc[i, \"year\"])\n",
    "            indicies.append(i)\n",
    "            \n",
    "    years = list(set(years))\n",
    "    df = pd.DataFrame(X_test.loc[indicies])\n",
    "    df = df.sort_values(by=\"year\")\n",
    "    \n",
    "    pred = pd.Series(y_predicted, index=X_test.index)\n",
    "    \n",
    "    y_resids.append(abs(y_test[df.index] - pred[df.index]).sum())\n",
    "    pythep_resids.append(abs(y_test[df.index] - df[\"pythep\"]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.152124003727124, 67.240972387475225)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_resids), np.mean(pythep_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "line": {
          "color": "rgb(0,0,0)"
         },
         "name": "Actual Wins",
         "type": "scatter",
         "x": [
          1996,
          1997,
          1998,
          1999,
          2000,
          2001,
          2002,
          2003,
          2004,
          2005,
          2006,
          2007,
          2008,
          2009,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016
         ],
         "y": [
          75,
          79,
          114,
          67,
          82,
          93,
          55,
          83,
          68,
          77,
          79,
          73,
          86,
          59,
          91,
          91,
          69,
          66,
          96,
          100,
          89
         ]
        },
        {
         "line": {
          "color": "rgb(0,0,255)"
         },
         "name": "Pythagorean Expectation",
         "type": "scatter",
         "x": [
          1996,
          1997,
          1998,
          1999,
          2000,
          2001,
          2002,
          2003,
          2004,
          2005,
          2006,
          2007,
          2008,
          2009,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016
         ],
         "y": [
          75.88930706220938,
          77.47992442135097,
          112.15384615384615,
          63.89412861136997,
          88.03018867924528,
          89.16100142479137,
          49.84615384615384,
          77.94448398576512,
          72.0276923076923,
          64.30613997423788,
          84.17524980784012,
          72.49723756906077,
          86.22037422037423,
          64.63973619126133,
          93.38823529411766,
          92.7905325443787,
          72.49723756906077,
          68.91796146674953,
          97.86575342465754,
          98.78048780487805,
          84.59822309970382
         ]
        },
        {
         "line": {
          "color": "rgb(255,0,0)"
         },
         "name": "Our Prediction",
         "type": "scatter",
         "x": [
          1996,
          1997,
          1998,
          1999,
          2000,
          2001,
          2002,
          2003,
          2004,
          2005,
          2006,
          2007,
          2008,
          2009,
          2010,
          2011,
          2012,
          2013,
          2014,
          2015,
          2016
         ],
         "y": [
          76.53227024281469,
          79.19399080680816,
          113.65577040257293,
          61.241032207798995,
          88.00249059034789,
          88.65165579642255,
          51.63041989194452,
          75.76535066118204,
          69.1624726639304,
          62.86370512629085,
          84.43416094614963,
          71.68898370218812,
          85.08302888246848,
          65.13804390838885,
          91.70883931013563,
          90.96375317132377,
          72.58583047971949,
          71.15384319673865,
          95.63819011799326,
          93.12988605161475,
          83.83424350898031
         ]
        }
       ],
       "layout": {
        "title": "Comparing Our Model to Pythagorean Expectation",
        "xaxis": {
         "title": "Year"
        },
        "yaxis": {
         "title": "Number of Wins"
        }
       }
      },
      "text/html": [
       "<div id=\"5cb18f58-c42b-46f3-934d-ed814b5da6c0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5cb18f58-c42b-46f3-934d-ed814b5da6c0\", [{\"line\": {\"color\": \"rgb(0,0,0)\"}, \"name\": \"Actual Wins\", \"type\": \"scatter\", \"y\": [75, 79, 114, 67, 82, 93, 55, 83, 68, 77, 79, 73, 86, 59, 91, 91, 69, 66, 96, 100, 89], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}, {\"line\": {\"color\": \"rgb(0,0,255)\"}, \"name\": \"Pythagorean Expectation\", \"type\": \"scatter\", \"y\": [75.88930706220938, 77.47992442135097, 112.15384615384615, 63.89412861136997, 88.03018867924528, 89.16100142479137, 49.84615384615384, 77.94448398576512, 72.0276923076923, 64.30613997423788, 84.17524980784012, 72.49723756906077, 86.22037422037423, 64.63973619126133, 93.38823529411766, 92.7905325443787, 72.49723756906077, 68.91796146674953, 97.86575342465754, 98.78048780487805, 84.59822309970382], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}, {\"line\": {\"color\": \"rgb(255,0,0)\"}, \"name\": \"Our Prediction\", \"type\": \"scatter\", \"y\": [76.53227024281469, 79.19399080680816, 113.65577040257293, 61.241032207798995, 88.00249059034789, 88.65165579642255, 51.63041989194452, 75.76535066118204, 69.1624726639304, 62.86370512629085, 84.43416094614963, 71.68898370218812, 85.08302888246848, 65.13804390838885, 91.70883931013563, 90.96375317132377, 72.58583047971949, 71.15384319673865, 95.63819011799326, 93.12988605161475, 83.83424350898031], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}], {\"title\": \"Comparing Our Model to Pythagorean Expectation\", \"xaxis\": {\"title\": \"Year\"}, \"yaxis\": {\"title\": \"Number of Wins\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"5cb18f58-c42b-46f3-934d-ed814b5da6c0\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"5cb18f58-c42b-46f3-934d-ed814b5da6c0\", [{\"line\": {\"color\": \"rgb(0,0,0)\"}, \"name\": \"Actual Wins\", \"type\": \"scatter\", \"y\": [75, 79, 114, 67, 82, 93, 55, 83, 68, 77, 79, 73, 86, 59, 91, 91, 69, 66, 96, 100, 89], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}, {\"line\": {\"color\": \"rgb(0,0,255)\"}, \"name\": \"Pythagorean Expectation\", \"type\": \"scatter\", \"y\": [75.88930706220938, 77.47992442135097, 112.15384615384615, 63.89412861136997, 88.03018867924528, 89.16100142479137, 49.84615384615384, 77.94448398576512, 72.0276923076923, 64.30613997423788, 84.17524980784012, 72.49723756906077, 86.22037422037423, 64.63973619126133, 93.38823529411766, 92.7905325443787, 72.49723756906077, 68.91796146674953, 97.86575342465754, 98.78048780487805, 84.59822309970382], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}, {\"line\": {\"color\": \"rgb(255,0,0)\"}, \"name\": \"Our Prediction\", \"type\": \"scatter\", \"y\": [76.53227024281469, 79.19399080680816, 113.65577040257293, 61.241032207798995, 88.00249059034789, 88.65165579642255, 51.63041989194452, 75.76535066118204, 69.1624726639304, 62.86370512629085, 84.43416094614963, 71.68898370218812, 85.08302888246848, 65.13804390838885, 91.70883931013563, 90.96375317132377, 72.58583047971949, 71.15384319673865, 95.63819011799326, 93.12988605161475, 83.83424350898031], \"x\": [1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]}], {\"title\": \"Comparing Our Model to Pythagorean Expectation\", \"xaxis\": {\"title\": \"Year\"}, \"yaxis\": {\"title\": \"Number of Wins\"}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotly: plot actual wins, Pythagorean Expectation predicted wins, and our predicted wins.\n",
    "trace0 = Scatter(\n",
    "    x = df['year'],\n",
    "    y = y_test[df.index],\n",
    "    name = \"Actual Wins\",\n",
    "    line = dict(\n",
    "        color = ('rgb(0,0,0)')\n",
    "    )\n",
    ")\n",
    "trace1 = Scatter(\n",
    "    x = df['year'],\n",
    "    y = df[\"pythep\"],\n",
    "    name = \"Pythagorean Expectation\",\n",
    "    line = dict(\n",
    "        color = ('rgb(0,0,255)')\n",
    "    )\n",
    ")\n",
    "trace2 = Scatter(\n",
    "    x = df['year'],\n",
    "    y = pred[df.index],\n",
    "    name = \"Our Prediction\",\n",
    "    line = dict(\n",
    "        color = ('rgb(255,0,0)')\n",
    "    )\n",
    ")\n",
    "\n",
    "traces = [trace0, trace1, trace2]\n",
    "\n",
    "layout = dict(\n",
    "    title = \"Comparing Our Model to Pythagorean Expectation\",\n",
    "    xaxis = dict(title = \"Year\"),\n",
    "    yaxis = dict(title = \"Number of Wins\")\n",
    ")\n",
    "\n",
    "fig = dict(data=traces, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our residual sizes are always very close to those that the Pythagorean Expectation produces. Machine learning does prove to be helpful for predicting baseball wins."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
